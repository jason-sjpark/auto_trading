# scripts/make_training_data.py
import os
import glob
import pandas as pd
import numpy as np

from feature_engineering.feature_pipeline import BatchFeaturePipeline
from feature_engineering.external_features import build_external_features
from labeling.scalping_labeler import make_scalping_labels

# ============================
# ì„¤ì • (í•„ìš”ì‹œ ìˆ˜ì •)
# ============================
OHLCV_PATH = "data/processed/ohlcv_1s_20251017.parquet"   # ìƒ˜í”Œ ê²½ë¡œ
TRADES_PATH = "data/raw/aggTrades_20251017.parquet"
DEPTH_PATH  = "data/raw/depth_20251017.parquet"
OUT_PATH    = "data/features/features_1s.parquet"

# ë¼ë²¨ë§ ì„¤ì •
LABEL_HORIZON = 5           # Nì´ˆ ë’¤
LABEL_THRESHOLD_PCT = 0.05  # 5% ê¸°ì¤€ (ì˜ˆ: ìŠ¤ì¼€ì¼ í™•ì¸ í›„ ì¡°ì •)

# asof ë³‘í•© í—ˆìš© ì˜¤ì°¨ ì‚¬ë‹¤ë¦¬
ASOF_TOLERANCES = [
    pd.Timedelta(milliseconds=500),
    pd.Timedelta(seconds=2),
    pd.Timedelta(seconds=5),
    pd.Timedelta(seconds=10),
]


# ============================
# ìœ í‹¸
# ============================
def _tz_normalize(df: pd.DataFrame, col: str = "timestamp") -> pd.DataFrame:
    """
    - ëª¨ë“  tsë¥¼ tz-naive UTC ê¸°ì¤€ìœ¼ë¡œ í†µì¼
    - ë¬¸ìì—´/epoch/ms í˜¼ì¬ë„ í¡ìˆ˜ (ìƒìœ„ì—ì„œ ms->datetimeìœ¼ë¡œ ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš°ë„ ì•ˆì „)
    - ì •ë ¬/ì¤‘ë³µ ì œê±°ê¹Œì§€ ìˆ˜í–‰
    """
    if df is None or df.empty:
        return pd.DataFrame(columns=[col])

    s = pd.to_datetime(df[col], utc=True, errors="coerce")
    # tz-aware â†’ UTCë¡œ ë§ì¶˜ ë’¤ tz ì œê±°(naive)
    if hasattr(s, "dt"):
        s = s.dt.tz_convert("UTC").dt.tz_localize(None)
    else:
        # ë‹¨ì¼ Timestampì¼ ê²½ìš°
        s = s.tz_convert("UTC").tz_localize(None)

    out = df.copy()
    out[col] = s
    out = out.dropna(subset=[col]).drop_duplicates(subset=[col]).sort_values(col)
    out.reset_index(drop=True, inplace=True)
    return out


def _print_range(name: str, df: pd.DataFrame, col: str = "timestamp"):
    if df is None or df.empty or col not in df.columns:
        print(f"âš ï¸ {name} EMPTY")
        return
    print(f"â± {name} range: {df[col].min()}  â†’  {df[col].max()}  ({len(df)} rows)")


def _latest(pattern: str):
    files = sorted(glob.glob(pattern))
    return files[-1] if files else None


def _safe_read_parquet(path: str, expected_cols=None) -> pd.DataFrame:
    if path is None:
        return pd.DataFrame(columns=expected_cols or ["timestamp"])
    try:
        df = pd.read_parquet(path)
        if expected_cols:
            # ìŠ¤í‚¤ë§ˆ ìµœì†Œí•œ ë³´ì •
            for c in expected_cols:
                if c not in df.columns:
                    df[c] = np.nan
        return df
    except Exception as e:
        print(f"âš ï¸ read_parquet fail: {path} ({e})")
        return pd.DataFrame(columns=expected_cols or ["timestamp"])


def _try_asof_merge(left: pd.DataFrame, right: pd.DataFrame, tolerances) -> pd.DataFrame:
    """
    asof mergeë¥¼ tolerance ì‚¬ë‹¤ë¦¬ë¡œ ì‹œë„. í•˜ë‚˜ë¼ë„ ì„±ê³µí•˜ë©´ ë°˜í™˜.
    ë‘˜ ì¤‘ í•˜ë‚˜ê°€ ë¹„ë©´ left ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    if left is None or left.empty:
        return left
    if right is None or right.empty:
        return left

    for tol in tolerances:
        merged = pd.merge_asof(
            left.sort_values("timestamp"),
            right.sort_values("timestamp"),
            on="timestamp",
            direction="nearest",
            tolerance=tol,
        )
        rows = len(merged.dropna(how="all"))
        print(f"ğŸ§ª merge_asof with tolerance={tol}: result rows = {len(merged)}")
        # ì´ ì¡°ê±´ì€ ë‹¨ìˆœíˆ ë³‘í•©ì´ ìˆ˜í–‰ëëŠ”ì§€ í™•ì¸ìš©. í•„ìš”ì‹œ ì¡°ê±´ ê°•í™” ê°€ëŠ¥.
        if len(merged) > 0:
            print(f"âœ… merged with tolerance={tol}")
            return merged
    print("âš ï¸ asof merge did not produce rows with given tolerances; returning last attempt")
    return merged  # ë§ˆì§€ë§‰ ì‹œë„ ê²°ê³¼ ë°˜í™˜


# ============================
# ë©”ì¸
# ============================
if __name__ == "__main__":
    print("ğŸ“¥ Loading source data...")

    # 1) ì›ì²œ ë°ì´í„° ë¡œë“œ
    ohlcv  = _safe_read_parquet(OHLCV_PATH, expected_cols=["timestamp","open","high","low","close","volume"])
    trades = _safe_read_parquet(TRADES_PATH, expected_cols=["timestamp","price","qty","side","trade_id","is_buyer_maker"])
    depth  = _safe_read_parquet(DEPTH_PATH,  expected_cols=["timestamp","bids","asks"])

    # í‘œì¤€í™”(íƒ€ì„ì¡´/ì •ë ¬/ì¤‘ë³µ)
    ohlcv  = _tz_normalize(ohlcv,  "timestamp")
    trades = _tz_normalize(trades, "timestamp")
    depth  = _tz_normalize(depth,  "timestamp")

    _print_range("OHLCV", ohlcv)
    _print_range("TRADES", trades)
    _print_range("DEPTH", depth)

    # 2) ì™¸ë¶€ ì§€í‘œ ë¡œë“œ â†’ ì™¸ë¶€ í”¼ì²˜ ìƒì„±
    ext_inputs = {
        "funding":       _safe_read_parquet(_latest("data/external/funding_rates_*.parquet")),
        "open_interest": _safe_read_parquet(_latest("data/external/open_interest_*.parquet")),
        "long_short_ratio": _safe_read_parquet(_latest("data/external/long_short_ratio_*.parquet")),
        "index_mark":    _safe_read_parquet(_latest("data/external/index_mark_*.parquet")),
        "liquidations":  _safe_read_parquet(_latest("data/external/liquidations_*.parquet")),
        "arbitrage":     _safe_read_parquet(_latest("data/external/arbitrage_spreads_*.parquet")),
    }
    external_df = build_external_features(ext_inputs)
    if external_df is None:
        external_df = pd.DataFrame(columns=["timestamp"])
    external_df = _tz_normalize(external_df, "timestamp")
    print("â± EXTERNAL range:",
          (external_df["timestamp"].min() if not external_df.empty else None),
          "â†’",
          (external_df["timestamp"].max() if not external_df.empty else None),
          (f"(rows {len(external_df)})" if not external_df.empty else "(empty)"))

    # 3) í”¼ì²˜ ìƒì„± (í˜¸ê°€/ì²´ê²°/ê¸°ìˆ ì§€í‘œ + ì™¸ë¶€í”¼ì²˜ ë³‘í•©)
    pipe = BatchFeaturePipeline(timeframes=["0.5s", "1s", "5s"])
    features_df = pipe.build_features(
        ohlcv_df=ohlcv,
        trades_df=trades,
        depth_df=depth,
        external_df=external_df,   # âœ… ì™¸ë¶€ í”¼ì²˜ ì£¼ì…
    )

    # ì•ˆì „ ì²˜ë¦¬: ì¤‘ë³µ ì»¬ëŸ¼ ì œê±° + ts í‘œì¤€í™”
    features_df = features_df.loc[:, ~features_df.columns.duplicated(keep="first")]
    features_df = _tz_normalize(features_df, "timestamp")
    _print_range("FEATURES", features_df)
    if features_df is None or features_df.empty:
        print("âš ï¸ FEATURES EMPTY")

    # 4) ë¼ë²¨ ìƒì„± (OHLCV ê¸°ë°˜)
    labeled = make_scalping_labels(ohlcv, horizon=LABEL_HORIZON, threshold_pct=LABEL_THRESHOLD_PCT)
    labeled = labeled[["timestamp", "label", "future_return"]]
    labeled = labeled.loc[:, ~labeled.columns.duplicated(keep="first")]
    labeled = _tz_normalize(labeled, "timestamp")
    _print_range("LABELS", labeled)

    # 5) í”¼ì²˜-ë¼ë²¨ ë³‘í•© (asof ì‚¬ë‹¤ë¦¬ â†’ ìµœê·¼ì ‘ fallback)
    final_dataset = None
    if not features_df.empty and not labeled.empty:
        # 1ì°¨: asof ì‚¬ë‹¤ë¦¬
        final_dataset = _try_asof_merge(features_df, labeled, ASOF_TOLERANCES)

        # 2ì°¨: ì—¬ì „íˆ 0í–‰ì´ë©´ ìµœê·¼ì ‘ ì´ì›ƒ ë§¤ì¹­ìœ¼ë¡œ ê°•ì œ ê²°í•©
        if final_dataset is None or len(final_dataset) == 0 or final_dataset[["label","future_return"]].isna().all().all():
            print("âš ï¸ asof merge still empty or labels missing. Falling back to nearest-neighbor labeling...")

            feat_ts = features_df["timestamp"].to_numpy()
            lab_ts = labeled["timestamp"].to_numpy()

            if len(feat_ts) > 0 and len(lab_ts) > 0:
                pos = np.searchsorted(lab_ts, feat_ts)
                pos = np.clip(pos, 1, len(lab_ts) - 1)
                prev = lab_ts[pos - 1]
                nxt = lab_ts[pos]
                # ì–´ëŠ ìª½ì´ ë” ê°€ê¹Œìš´ì§€ ì„ íƒ
                pick = np.where((feat_ts - prev) <= (nxt - feat_ts), prev, nxt)
                nn_lab = labeled.set_index("timestamp").loc[pick, ["label", "future_return"]].reset_index(drop=True)
                final_dataset = pd.concat([features_df.reset_index(drop=True), nn_lab], axis=1)
                print(f"âœ… fallback merged rows = {len(final_dataset)}")
            else:
                final_dataset = features_df.copy()
                final_dataset["label"] = np.nan
                final_dataset["future_return"] = np.nan
                print("âš ï¸ not enough timestamps for NN fallback; labels set to NaN.")
    else:
        # í”¼ì²˜ ë˜ëŠ” ë¼ë²¨ì´ ë¹„ì–´ë„ ìŠ¤í‚¤ë§ˆë¥¼ ìœ ì§€í•˜ë„ë¡ ì²˜ë¦¬
        final_dataset = features_df.copy()
        if "label" not in final_dataset.columns:
            final_dataset["label"] = np.nan
        if "future_return" not in final_dataset.columns:
            final_dataset["future_return"] = np.nan
        print("âš ï¸ features or labels empty; produced dataset with NaN labels.")

    # 6) ìµœì¢… ì €ì¥
    final_dataset = final_dataset.loc[:, ~final_dataset.columns.duplicated(keep="first")]
    os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)
    final_dataset.to_parquet(OUT_PATH, index=False)

    print("ğŸ‰ DONE: saved â†’", OUT_PATH)
    print("shape:", final_dataset.shape)
    print("sample:")
    print(final_dataset.head(3))
