# scripts/make_training_data.py
"""
í†µí•© í•™ìŠµ ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸ (ì „ì—­ safemode ìŠ¤ìœ„ì¹˜)

- ì…ë ¥:
  * OHLCV(1s): data/processed/ohlcv_1s_YYYYMMDD.parquet
  * Trades raw: data/raw/aggTrades/{SYMBOL}/YYYY-MM-DD.parquet
  * Depth raw : data/raw/depth/{SYMBOL}/YYYY-MM-DD.parquet  (ì§‘ê³„í˜• bookDepth í”¼ë²— ì™„ë£Œë³¸)
  * External  : data/external/*.parquet (funding, oi, lsr, index/mark, arb)
  * Liquidations(ì˜µì…˜): data/realtime/liquidations/{SYMBOL}/**/*.parquet

- ì¶œë ¥:
  * data/features/features_1s.parquet

- ë¼ë²¨:
  * horizon_s=5, threshold_bp=10 (Â±0.10%), neutral í¬í•¨

- safemode (ì „ì—­ ë³€ìˆ˜):
  * True  â†’ 15ë¶„ í´ë¦½ìœ¼ë¡œ ë¹ ë¥¸ ê²€ì¦
  * False â†’ ì „ì²´ êµ¬ê°„
"""

import os
import sys
import glob
import time
import argparse
from datetime import timedelta

import pandas as pd
import numpy as np

# ===== ì „ì—­ ìŠ¤ìœ„ì¹˜ =====
safemode: bool = False   # â† ì—¬ê¸°ë§Œ True/False ë¡œ ë°”ê¿”ì„œ ì‚¬ìš©

# --- ë¡œì»¬ ëª¨ë“ˆ ---
from feature_engineering.feature_pipeline import BatchFeaturePipeline
from feature_engineering.external_features import build_external_features
from labeling.scalping_labeler import make_scalping_labels


# -------------------------
# ìœ í‹¸
# -------------------------
def _to_dt_utc_naive(s) -> pd.Series:
    ts = pd.to_datetime(s, utc=True, errors="coerce")
    return ts.dt.tz_convert("UTC").dt.tz_localize(None)

def _print_range(name: str, df: pd.DataFrame, col: str = "timestamp"):
    if df is None or df.empty or col not in df.columns:
        print(f"âš ï¸ {name} EMPTY")
        return
    print(f"â± {name} range: {df[col].min()} â†’ {df[col].max()} ({len(df)} rows)")

def _latest_path(pattern: str) -> str:
    paths = glob.glob(pattern)
    if not paths:
        return ""
    paths.sort(key=lambda p: os.path.getmtime(p))
    return paths[-1]

def _detect_ohlcv(date: str = "", base="data/processed") -> str:
    if date:
        return os.path.join(base, f"ohlcv_1s_{date.replace('-', '')}.parquet")
    return _latest_path(os.path.join(base, "ohlcv_1s_*.parquet"))

def _detect_trades(symbol: str, date: str, base="data/raw/aggTrades") -> str:
    return os.path.join(base, symbol, f"{date}.parquet")

def _detect_depth(symbol: str, date: str, base="data/raw/depth") -> str:
    return os.path.join(base, symbol, f"{date}.parquet")

def _load_parquet_safe(path: str, required_cols=None) -> pd.DataFrame:
    try:
        if not path or not os.path.exists(path):
            return pd.DataFrame()
        df = pd.read_parquet(path)
        if required_cols:
            for c in required_cols:
                if c not in df.columns:
                    df[c] = np.nan
        return df
    except Exception as e:
        print(f"âš ï¸ read parquet failed: {path} â†’ {e}")
        return pd.DataFrame()

def _load_external_inputs() -> dict:
    base = "data/external"
    out = {}
    out["funding"]          = _load_parquet_safe(_latest_path(os.path.join(base, "funding_rates_*.parquet")))
    out["open_interest"]    = _load_parquet_safe(_latest_path(os.path.join(base, "open_interest_*.parquet")))
    out["long_short_ratio"] = _load_parquet_safe(_latest_path(os.path.join(base, "long_short_ratio_*.parquet")))
    out["index_mark"]       = _load_parquet_safe(_latest_path(os.path.join(base, "index_mark_*.parquet")))
    out["arbitrage"]        = _load_parquet_safe(_latest_path(os.path.join(base, "arbitrage_spreads_*.parquet")))
    return out

def _load_liquidations_between(symbol="BTCUSDT", start=None, end=None) -> pd.DataFrame:
    """ì‹¤ì‹œê°„ ì²­ì‚° ìŠ¤ëƒ…ìƒ·(WS ì €ì¥ë¶„) ë¡œë”©"""
    root = os.path.join("data", "realtime", "liquidations", symbol)
    frames = []
    for path in glob.glob(os.path.join(root, "**", "*.parquet"), recursive=True):
        try:
            df = pd.read_parquet(path)
            if "timestamp" not in df.columns:
                continue
            df["timestamp"] = _to_dt_utc_naive(df["timestamp"])
            if start is not None:
                df = df[df["timestamp"] >= start]
            if end is not None:
                df = df[df["timestamp"] <= end]
            if not df.empty:
                cols = [c for c in ["timestamp", "side", "price", "qty", "notional"] if c in df.columns]
                frames.append(df[cols])
        except Exception:
            pass
    if not frames:
        return pd.DataFrame(columns=["timestamp", "side", "price", "qty", "notional"])
    out = pd.concat(frames, ignore_index=True).sort_values("timestamp")
    return out

def _detect_rt_best(symbol: str, date: str, base="data/processed/realtime_depth_best") -> str:
    # date: "YYYY-MM-DD"
    return os.path.join(base, symbol, f"{date}.parquet")

def _load_rt_best(symbol: str, date: str):
    p = _detect_rt_best(symbol, date)
    if not os.path.exists(p):
        return pd.DataFrame()
    df = pd.read_parquet(p)
    if "timestamp" not in df.columns:
        return pd.DataFrame()
    df["timestamp"] = _to_dt_utc_naive(df["timestamp"])
    keep = [c for c in [
        "timestamp","best_bid","best_ask","best_bid_sz","best_ask_sz","spread","mid_price","ob_imbalance"
    ] if c in df.columns]
    return df[keep].dropna(subset=["timestamp"]).sort_values("timestamp")


# -------------------------
# ë©”ì¸
# -------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--symbol", default="BTCUSDT")
    ap.add_argument("--date", default="", help="YYYY-MM-DD (ë¯¸ì§€ì • ì‹œ ìµœì‹  OHLCV ë‚ ì§œë¡œ ì¶”ì •)")
    ap.add_argument("--out", default="data/features/features_1s.parquet")
    ap.add_argument("--horizon_s", type=int, default=5)
    ap.add_argument("--threshold_bp", type=float, default=10.0)
    args = ap.parse_args()

    symbol = args.symbol
    horizon_s = int(args.horizon_s)
    threshold_bp = float(args.threshold_bp)

    # --- 1) ì…ë ¥ ê²½ë¡œ íƒìƒ‰/ë¡œë“œ ---
    t0 = time.time()
    ohlcv_path = _detect_ohlcv(args.date)
    if not ohlcv_path:
        print("âŒ OHLCV íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¨¼ì € backfill ë˜ëŠ” ì‹¤ì‹œê°„ ìˆ˜ì§‘ìœ¼ë¡œ 1s OHLCVë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        sys.exit(1)

    # date ì¶”ì¶œ
    if args.date:
        date_str = args.date
    else:
        base = os.path.basename(ohlcv_path)  # ohlcv_1s_YYYYMMDD.parquet
        date_str = base.replace("ohlcv_1s_", "").replace(".parquet", "")
        if len(date_str) == 8:
            date_str = f"{date_str[0:4]}-{date_str[4:6]}-{date_str[6:8]}"

    trades_path = _detect_trades(symbol, date_str)
    depth_path  = _detect_depth(symbol, date_str)

    print("ğŸ“¥ Loading source data...")
    ohlcv = _load_parquet_safe(ohlcv_path, required_cols=["timestamp", "open", "high", "low", "close", "volume"])
    ohlcv["timestamp"] = _to_dt_utc_naive(ohlcv["timestamp"])
    ohlcv = ohlcv.dropna(subset=["timestamp"]).sort_values("timestamp")
    _print_range("OHLCV", ohlcv)
    print(f"â³ load OHLCV done in {time.time()-t0:.2f}s")

    t1 = time.time()
    trades = _load_parquet_safe(trades_path)
    if not trades.empty:
        # í‘œì¤€í™”: timestamp/price/qty/side
        ts_col = "timestamp" if "timestamp" in trades.columns else ("time" if "time" in trades.columns else None)
        if ts_col:
            trades["timestamp"] = trades[ts_col]
        trades["timestamp"] = _to_dt_utc_naive(trades["timestamp"])
        trades["price"] = pd.to_numeric(trades.get("price", 0), errors="coerce")
        trades["qty"] = pd.to_numeric(trades.get("qty", 0), errors="coerce")
        if "side" not in trades.columns and "is_buyer_maker" in trades.columns:
            trades["side"] = np.where(trades["is_buyer_maker"], "sell", "buy")
        trades["side"] = trades.get("side", "buy").astype(str).str.lower()
        trades = trades.dropna(subset=["timestamp"]).sort_values("timestamp")
    _print_range("TRADES", trades)

    # --- DEPTH: â‘  ì§‘ê³„ depth ë¡œë“œ â†’ â‘¡ ì‹¤ì‹œê°„ BEST ìˆìœ¼ë©´ êµì²´ â†’ â‘¢ ë‘˜ ë‹¤ ì—†ìœ¼ë©´ placeholder ---
    depth = _load_parquet_safe(depth_path)
    if not depth.empty:
        if "timestamp" in depth.columns:
            depth["timestamp"] = _to_dt_utc_naive(depth["timestamp"])
            depth = depth.dropna(subset=["timestamp"]).sort_values("timestamp")

    rt_best = _load_rt_best(symbol, date_str)
    if not rt_best.empty:
        depth = rt_best  # ì§„ì§œ spread/mid í™•ë³´ìš©
        print("ğŸ” Using realtime BEST depth instead of aggregated bookDepth")

    if depth is None or depth.empty or "timestamp" not in depth.columns:
        # placeholder: OHLCV íƒ€ì„ìŠ¤íƒ¬í”„ì— ì •ë ¬ (í”¼ì²˜ íŒŒì´í”„ë¼ì¸ì´ ê¸°ëŒ€í•˜ëŠ” ì»¬ëŸ¼ ë¼ˆëŒ€ ì œê³µ)
        placeholder = pd.DataFrame({"timestamp": ohlcv["timestamp"].copy()})
        for c in ["best_bid","best_ask","best_bid_sz","best_ask_sz","spread","mid_price","ob_imbalance"]:
            placeholder[c] = np.nan
        depth = placeholder
        print("âš ï¸ No depth sources found. Created placeholder depth aligned to OHLCV.")

    _print_range("DEPTH(final)", depth)
    print(f"â³ load TRADES/DEPTH done in {time.time()-t1:.2f}s")

    # --- 1.5) safemode: 15ë¶„ í´ë¦½ ---
    # ê³µí†µ êµ¬ê°„ íƒìƒ‰ ì‹œ, ê° DFê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ê³„ì‚°
    cstart_candidates = []
    cend_candidates = []

    if not ohlcv.empty:
        cstart_candidates.append(ohlcv["timestamp"].min())
        cend_candidates.append(ohlcv["timestamp"].max())
    if not trades.empty:
        cstart_candidates.append(trades["timestamp"].min())
        cend_candidates.append(trades["timestamp"].max())
    if not depth.empty:
        cstart_candidates.append(depth["timestamp"].min())
        cend_candidates.append(depth["timestamp"].max())

    if cstart_candidates and cend_candidates:
        clip_start = max(cstart_candidates)
        clip_end   = min(cend_candidates)
    else:
        print("âŒ ì…ë ¥ ë°ì´í„°ê°€ ëª¨ë‘ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

    if pd.isna(clip_start) or pd.isna(clip_end) or clip_end <= clip_start:
        if not ohlcv.empty:
            clip_start = ohlcv["timestamp"].min()
            clip_end   = clip_start + timedelta(minutes=15)
        else:
            print("âŒ ê²¹ì¹˜ëŠ” ì‹œê°„ êµ¬ê°„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            sys.exit(1)

    if safemode:
        _clip_end = min(clip_end, clip_start + timedelta(minutes=15))
        print(f"ğŸ”’ SAFE MODE: restricting window to 15 minutes â†’ {clip_start} â†’ {_clip_end}")
        ohlcv  = ohlcv[(ohlcv["timestamp"] >= clip_start) & (ohlcv["timestamp"] <= _clip_end)]
        trades = trades[(trades["timestamp"] >= clip_start) & (trades["timestamp"] <= _clip_end)]
        depth  = depth[(depth["timestamp"]  >= clip_start) & (depth["timestamp"]  <= _clip_end)]
        _print_range("OHLCV(clipped)", ohlcv)
        _print_range("TRADES(clipped)", trades)
        _print_range("DEPTH(clipped)", depth)
        print("â³ clip windows done in {:.2f}s".format(time.time()-t1))

    # --- 2) ì™¸ë¶€ì§€í‘œ + ë¦¬í€´ë°ì´ì…˜ í†µí•© ---
    t2 = time.time()
    ext_inputs = _load_external_inputs()
    liq_df = _load_liquidations_between(symbol=symbol, start=ohlcv["timestamp"].min(), end=ohlcv["timestamp"].max())
    if not liq_df.empty:
        ext_inputs["liquidations"] = liq_df

    print("â³ load external inputs done in {:.2f}s".format(time.time()-t2))
    ext_all = []
    for k, v in ext_inputs.items():
        if v is not None and not v.empty and "timestamp" in v.columns:
            ext_all.append(v[["timestamp"]])
    if ext_all:
        tmp = pd.concat(ext_all, ignore_index=True)
        tmp["timestamp"] = _to_dt_utc_naive(tmp["timestamp"])
        if not tmp.empty:
            print("â± EXTERNAL range: {} â†’ {} ({} rows)".format(tmp["timestamp"].min(), tmp["timestamp"].max(), len(tmp)))
    external_df = build_external_features(
        ext_inputs,
        liq_windows_s=(30, 60),
        big_liq_notional=100000.0
    )
    if external_df is None:
        external_df = pd.DataFrame(columns=["timestamp"])
    print("â³ build external_df done in {:.2f}s".format(time.time()-t2))

    # --- 3) í”¼ì²˜ ìƒì„± ---
    t3 = time.time()
    pipe = BatchFeaturePipeline(timeframes=["0.5s", "1s", "5s"])
    try:
        features_df = pipe.build_features(
            ohlcv_df=ohlcv,
            trades_df=trades,
            depth_df=depth,
            external_df=external_df
        )
    except Exception as e:
        print(f"âŒ pipe.build_features failed: {e}")
        raise

    if features_df is None or features_df.empty:
        print("âš ï¸ [build_features] ê²°ê³¼ DataFrameì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        print("ohlcv_df:", ohlcv.shape if ohlcv is not None else None)
        print("trades_df:", trades.shape if trades is not None else None)
        print("depth_df:", depth.shape if depth is not None else None)
    else:
        features_df = features_df.loc[:, ~features_df.columns.duplicated(keep="first")]
        if "timestamp" in features_df.columns:
            features_df["timestamp"] = _to_dt_utc_naive(features_df["timestamp"])
            features_df = features_df.dropna(subset=["timestamp"]).sort_values("timestamp")
        features_df = features_df.ffill().bfill()

    print("âœ… [build_features] shape={}".format(features_df.shape if features_df is not None else None))
    print("â³ pipe.build_features done in {:.2f}s".format(time.time()-t3))
    _print_range("FEATURES", features_df)
    print("â³ post features done in {:.2f}s".format(time.time()-t3))

    # --- 4) ë¼ë²¨ ìƒì„± ---
    t4 = time.time()
    labeled = make_scalping_labels(
        ohlcv_df=ohlcv,
        horizon_s=horizon_s,
        threshold_bp=threshold_bp,
        neutral_band=True,
        price_col="close",
    )
    labeled = labeled[["timestamp", "label", "future_return"]]
    labeled["timestamp"] = _to_dt_utc_naive(labeled["timestamp"])
    labeled = labeled.dropna(subset=["timestamp"]).sort_values("timestamp")
    _print_range("LABELS", labeled)
    print("â³ make labels done in {:.2f}s".format(time.time()-t4))

    # --- 5) ë³‘í•© ---
    t5 = time.time()
    if features_df is None or features_df.empty:
        final_dataset = pd.DataFrame(columns=["timestamp", "label", "future_return"])
    else:
        f = features_df.sort_values("timestamp")
        l = labeled.sort_values("timestamp")
        final_dataset = pd.merge_asof(
            f, l, on="timestamp", direction="nearest",
            tolerance=pd.Timedelta(milliseconds=500)
        )
        if final_dataset is None or final_dataset.empty:
            print("âš ï¸ merge_asof ê²°ê³¼ê°€ ë¹„ì–´ fallback nearest-neighbor labeling...")
            lab_ts = l["timestamp"].to_numpy()
            feat_ts = f["timestamp"].to_numpy()
            if len(lab_ts) == 0 or len(feat_ts) == 0:
                final_dataset = f.copy()
                final_dataset["label"] = 0
                final_dataset["future_return"] = 0.0
            else:
                pos = np.searchsorted(lab_ts, feat_ts)
                pos = np.clip(pos, 1, len(lab_ts) - 1)
                prev = lab_ts[pos - 1]
                next = lab_ts[pos]
                pick = np.where((feat_ts - prev) <= (next - feat_ts), prev, next)
                nn_lab = l.set_index("timestamp").loc[pick, ["label", "future_return"]].reset_index(drop=True)
                final_dataset = pd.concat([f.reset_index(drop=True), nn_lab], axis=1)

    final_dataset = final_dataset.loc[:, ~final_dataset.columns.duplicated(keep="first")]
    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    final_dataset.to_parquet(args.out, index=False)


    # --- Quick quality report (optional) ---
    critical_cols = ["spread","mid_price","orderbook_imbalance","liquidity_gap","wall_strength","depth_balance"]
    null_rates = final_dataset[critical_cols].isna().mean().sort_values(ascending=False)
    print("ğŸ§ª null rates (critical):")
    print((null_rates*100).round(2).astype(str) + "%")

    print("â³ merge labels done in {:.2f}s".format(time.time()-t5))
    print(f"ğŸ‰ DONE: {args.out}")
    print("shape:", final_dataset.shape)
    print(final_dataset.head(3))
    print("âœ… ALL DONE in {:.2f}s (safemode={})".format(time.time()-t0, safemode))


if __name__ == "__main__":
    main()
