# scripts/make_training_data.py
import os
import pandas as pd
import numpy as np

from feature_engineering.feature_pipeline import BatchFeaturePipeline
from labeling.scalping_labeler import make_scalping_labels

# -------------------------
# ê²½ë¡œ ì„¤ì • (í•„ìš”ì‹œ ìˆ˜ì •)
# -------------------------
OHLCV_PATH = "data/processed/ohlcv_1s_20251017.parquet"
TRADES_PATH = "data/raw/aggTrades_20251017.parquet"
DEPTH_PATH  = "data/raw/depth_20251017.parquet"
OUT_PATH    = "data/features/features_1s.parquet"

# -------------------------
# ìœ í‹¸: íƒ€ì„ìŠ¤íƒ¬í”„ í‘œì¤€í™”
# -------------------------
def _tz_normalize(df: pd.DataFrame, col: str = "timestamp") -> pd.DataFrame:
    """
    - ëª¨ë“  tsë¥¼ tz-naive UTC ê¸°ì¤€ìœ¼ë¡œ í†µì¼
    - ë¬¸ìì—´/epoch í˜¼ì¬ë„ í¡ìˆ˜
    - ì •ë ¬/ì¤‘ë³µ ì œê±°ê¹Œì§€ ìˆ˜í–‰
    """
    s = pd.to_datetime(df[col], utc=True, errors="coerce")
    # tz-aware â†’ UTCë¡œ ë§ì¶˜ ë’¤ tz ì œê±°(naive)
    s = s.dt.tz_convert("UTC").dt.tz_localize(None)
    df = df.copy()
    df[col] = s
    # ì •ë ¬ + ì¤‘ë³µ ì œê±°
    df = df.sort_values(col)
    df = df[~df[col].isna()].drop_duplicates(subset=[col])
    return df

def _print_range(name: str, df: pd.DataFrame, col: str = "timestamp"):
    if df.empty:
        print(f"âš ï¸ {name} EMPTY")
    else:
        print(f"â± {name} range: {df[col].min()}  â†’  {df[col].max()}  ({len(df)} rows)")

# -------------------------
# 1) ë¡œë“œ
# -------------------------
print("ğŸ“¥ Loading source data...")
ohlcv  = pd.read_parquet(OHLCV_PATH)
trades = pd.read_parquet(TRADES_PATH)
depth  = pd.read_parquet(DEPTH_PATH)

# í‘œì¤€í™”(íƒ€ì„ì¡´/ì •ë ¬/ì¤‘ë³µ)
ohlcv  = _tz_normalize(ohlcv,  "timestamp")
trades = _tz_normalize(trades, "timestamp")
depth  = _tz_normalize(depth,  "timestamp")

_print_range("OHLCV", ohlcv)
_print_range("TRADES", trades)
_print_range("DEPTH", depth)

# -------------------------
# 2) í”¼ì²˜ ìƒì„±
# -------------------------
pipe = BatchFeaturePipeline(timeframes=["0.5s", "1s", "5s"])
features_df = pipe.build_features(
    ohlcv_df=ohlcv,
    trades_df=trades,
    depth_df=depth
)


# í‘œì¤€í™” + ì•ˆì „ë²¨íŠ¸
features_df = features_df.loc[:, ~features_df.columns.duplicated(keep="first")]
features_df = _tz_normalize(features_df, "timestamp")

_print_range("FEATURES", features_df)

# -------------------------
# 3) ë¼ë²¨ ìƒì„± (OHLCV ê¸°ë°˜)
# -------------------------
labeled = make_scalping_labels(ohlcv, horizon=5, threshold_pct=0.05)
labeled = labeled[["timestamp", "label", "future_return"]]
labeled = labeled.loc[:, ~labeled.columns.duplicated(keep="first")]
labeled = _tz_normalize(labeled, "timestamp")

_print_range("LABELS", labeled)

# -------------------------
# 4) ë³‘í•© ë¡œì§ (ë‹¨ê³„ì  ì‹œë„)
# -------------------------
def try_merge(tol):
    return pd.merge_asof(
        features_df.sort_values("timestamp"),
        labeled.sort_values("timestamp"),
        on="timestamp",
        direction="nearest",
        tolerance=tol
    )

# 4-1) tolerance ì‚¬ë‹¤ë¦¬: 0.5s â†’ 2s â†’ 5s â†’ 10s
tolerances = [pd.Timedelta(milliseconds=500),
              pd.Timedelta(seconds=2),
              pd.Timedelta(seconds=5),
              pd.Timedelta(seconds=10)]

final_dataset = None
for tol in tolerances:
    tmp = try_merge(tol)
    print(f"ğŸ§ª merge_asof with tolerance={tol}: result rows = {len(tmp)}")
    if len(tmp) > 0:
        final_dataset = tmp
        print(f"âœ… merged with tolerance={tol}")
        break

# 4-2) ê·¸ë˜ë„ 0í–‰ì´ë©´: ë¼ë²¨ì„ íŠ¹ì„± íƒ€ì„ìŠ¤íƒ¬í”„ì— ë³´ê°„(ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ)
if final_dataset is None or len(final_dataset) == 0:
    print("âš ï¸ asof merge still empty. Falling back to nearest-neighbor labeling...")

    # ë‘ ì¶•ì„ epoch(ms)ë¡œ ë°”ê¿”ì„œ ìµœê·¼ì ‘ ì¸ë±ìŠ¤ ë§¤ì¹­
    f_ms = features_df["timestamp"].view("int64") // 10**6
    l_ms = labeled["timestamp"].view("int64") // 10**6
    lf   = labeled.set_index(labeled["timestamp"])

    # ê° feature tsì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ label ts ì°¾ê¸°
    # (íš¨ìœ¨ì ìœ¼ë¡œëŠ” searchsortedë¥¼ ì‚¬ìš©)
    lab_ts = labeled["timestamp"].to_numpy()
    feat_ts = features_df["timestamp"].to_numpy()

    pos = np.searchsorted(lab_ts, feat_ts)
    pos = np.clip(pos, 1, len(lab_ts)-1)
    prev = lab_ts[pos-1]
    next = lab_ts[pos]
    # ì–´ëŠ ìª½ì´ ë” ê°€ê¹Œìš´ì§€
    pick = np.where((feat_ts - prev) <= (next - feat_ts), prev, next)

    nn_lab = labeled.set_index("timestamp").loc[pick, ["label", "future_return"]].reset_index(drop=True)
    final_dataset = pd.concat([features_df.reset_index(drop=True), nn_lab], axis=1)
    print(f"âœ… fallback merged rows = {len(final_dataset)}")

# -------------------------
# 5) ìµœì¢… ì €ì¥
# -------------------------
final_dataset = final_dataset.loc[:, ~final_dataset.columns.duplicated(keep="first")]
os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)
final_dataset.to_parquet(OUT_PATH, index=False)

print("ğŸ‰ DONE: saved â†’", OUT_PATH)
print("shape:", final_dataset.shape)
print("sample:")
print(final_dataset.head(3))
