from fastapi import FastAPI, Request
from pydantic import BaseModel
import onnxruntime as ort
import numpy as np
import joblib
import torch
import uvicorn
from typing import List, Dict

# ==============================================================
# âœ… í™˜ê²½ ì„¤ì •
# ==============================================================

MODEL_PATH = "models/outputs/scalping_lstm.onnx"
SCALER_PATH = "models/outputs/feature_scaler.pkl"
SEQ_LEN = 30

app = FastAPI(title="Scalping LSTM Inference API", version="1.0")

# ==============================================================
# ğŸ§  ëª¨ë¸ & ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
# ==============================================================

print("ğŸš€ Loading ONNX model...")
session = ort.InferenceSession(MODEL_PATH, providers=["CPUExecutionProvider"])

print("ğŸ”§ Loading feature scaler...")
scaler_bundle = joblib.load(SCALER_PATH)
scaler = scaler_bundle["scaler"]
feature_cols = scaler_bundle["feature_cols"]

# ==============================================================
# ğŸ§© ì…ë ¥ ìŠ¤í‚¤ë§ˆ
# ==============================================================

class FeatureWindow(BaseModel):
    """
    seq_len(ê¸°ë³¸ 30)ê°œì˜ ì‹œì ì´ í¬í•¨ëœ í”¼ì²˜ ìœˆë„ìš° ì…ë ¥
    """
    sequence: List[Dict[str, float]]


# ==============================================================
# âš™ï¸ í—¬í¼ í•¨ìˆ˜
# ==============================================================

def prepare_input(sequence: List[Dict[str, float]]):
    """
    JSON ì‹œí€€ìŠ¤ë¥¼ (1, seq_len, num_features)ë¡œ ë³€í™˜ + ìŠ¤ì¼€ì¼ë§
    """
    # í”¼ì²˜ ì •ë ¬ ì¼ê´€ì„± ìœ ì§€
    seq_array = np.array([[frame.get(f, 0.0) for f in feature_cols] for frame in sequence], dtype=np.float32)
    if seq_array.shape[0] < SEQ_LEN:
        pad_len = SEQ_LEN - seq_array.shape[0]
        pad = np.zeros((pad_len, seq_array.shape[1]), dtype=np.float32)
        seq_array = np.vstack([pad, seq_array])
    elif seq_array.shape[0] > SEQ_LEN:
        seq_array = seq_array[-SEQ_LEN:]

    # ìŠ¤ì¼€ì¼ë§
    seq_scaled = scaler.transform(seq_array)
    return seq_scaled[np.newaxis, :, :]  # (1, seq_len, F)


# ==============================================================
# ğŸ”® ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸
# ==============================================================

@app.post("/predict")
def predict(features: FeatureWindow):
    """
    ì…ë ¥: JSON { sequence: [ {feature1: val, feature2: val, ...}, ... ] }
    ì¶œë ¥: ìƒìŠ¹/í•˜ë½/íš¡ë³´ í™•ë¥ 
    """
    try:
        x = prepare_input(features.sequence)
        ort_inputs = {"input": x}
        logits = session.run(None, ort_inputs)[0]
        probs = torch.softmax(torch.tensor(logits), dim=-1).numpy().flatten()

        # ìˆœì„œ: [-1, 0, 1] (í•˜ë½, íš¡ë³´, ìƒìŠ¹)
        result = {
            "class_probs": {
                "down": float(probs[0]),
                "sideways": float(probs[1]),
                "up": float(probs[2])
            },
            "predicted_label": int(np.argmax(probs) - 1)  # 0â†’-1, 1â†’0, 2â†’1
        }
        return result

    except Exception as e:
        return {"error": str(e)}


@app.get("/")
def root():
    return {"message": "Scalping Model Inference API is running ğŸš€"}


# ==============================================================
# ğŸ ì„œë²„ ì‹¤í–‰
# ==============================================================

if __name__ == "__main__":
    uvicorn.run("models.serve.app:app", host="0.0.0.0", port=8000, reload=True)
